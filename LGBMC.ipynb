{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loc', 'v(g)', 'ev(g)', 'iv(g)', 'n', 'v', 'l', 'd', 'i', 'e', 'b', 't',\n",
      "       'lOCode', 'lOComment', 'lOBlank', 'locCodeAndComment', 'uniq_Op',\n",
      "       'uniq_Opnd', 'total_Op', 'total_Opnd', 'branchCount', 'defects'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./train.csv\").drop('id',axis=1)\n",
    "df_clean = df.drop('defects',axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_clean.columns:\n",
    "    df_clean[column] = np.log1p(df_clean.pop(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean\n",
    "y = df['defects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_cl_bo(min_child_samples, colsample_bytree, learning_rate, num_leaves, reg_alpha, reg_lambda):\n",
    "    \n",
    "    params_lgbm = {}\n",
    "    params_lgbm['min_child_samples'] = round(min_child_samples)\n",
    "    params_lgbm['colsample_bytree'] = colsample_bytree\n",
    "    params_lgbm['learning_rate'] = learning_rate\n",
    "    params_lgbm['num_leaves'] = round(num_leaves)\n",
    "    params_lgbm['reg_alpha'] = reg_alpha\n",
    "    params_lgbm['reg_lambda'] = reg_lambda    \n",
    "       \n",
    "    params_lgbm['boosting_type'] ='gbdt'   # Manual optimization\n",
    "    params_lgbm['objective'] ='binary'     # Manual optimization\n",
    "    params_lgbm['subsample'] = 1.0\n",
    "    params_lgbm['max_bin'] = 1023\n",
    "    params_lgbm['n_jobs'] = -1\n",
    "\n",
    "    scores = cross_val_score(LGBMClassifier(**params_lgbm, random_state=2920), X, y, scoring='roc_auc', cv=5).mean()\n",
    "    score = scores.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgbm ={'min_child_samples':(800, 1200),\n",
    "              'colsample_bytree':(0.3, 1.0),\n",
    "              'learning_rate':(0.005, 0.1),\n",
    "              'num_leaves':(20, 60),\n",
    "              'reg_alpha':(0.0, 10.0),\n",
    "              'reg_lambda':(0.0, 5.0)}\n",
    "\n",
    "lgbm_bo = BayesianOptimization(lgbm_cl_bo, params_lgbm, random_state=2920)\n",
    "lgbm_bo.maximize(n_iter=30, init_points=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5687937442159554,\n",
       " 'learning_rate': 0.07712378501966154,\n",
       " 'min_child_samples': 864.0979874652943,\n",
       " 'num_leaves': 23.238382472492944,\n",
       " 'reg_alpha': 0.6515291800331102,\n",
       " 'reg_lambda': 3.1182674170838522}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmax_bayes = lgbm_bo.max['params']\n",
    "pmax_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(n_estimators= 20000, \n",
    "                       learning_rate= 0.07,\n",
    "                       objective= 'binary', \n",
    "                       boosting_type= 'gbdt', \n",
    "                       \n",
    "                       subsample= 1.0,\n",
    "                       num_leaves= 23,  \n",
    "                       max_bin= 1023,\n",
    "                       n_jobs= -1,\n",
    "                           \n",
    "                       reg_alpha= 0.65,\n",
    "                       reg_lambda= 3.12,\n",
    "                       colsample_bytree= 0.568,\n",
    "                       min_child_samples= 864,     \n",
    "                       random_state= 1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 23064, number of negative: 78699\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8505\n",
      "[LightGBM] [Info] Number of data points in the train set: 101763, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226644 -> initscore=-1.227357\n",
      "[LightGBM] [Info] Start training from score -1.227357\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;quantiletransformer&#x27;,\n",
       "                 QuantileTransformer(output_distribution=&#x27;normal&#x27;)),\n",
       "                (&#x27;lgbmclassifier&#x27;,\n",
       "                 LGBMClassifier(colsample_bytree=0.568, learning_rate=0.07,\n",
       "                                max_bin=1023, min_child_samples=864,\n",
       "                                n_estimators=20000, n_jobs=-1, num_leaves=23,\n",
       "                                objective=&#x27;binary&#x27;, random_state=1920,\n",
       "                                reg_alpha=0.65, reg_lambda=3.12))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;quantiletransformer&#x27;,\n",
       "                 QuantileTransformer(output_distribution=&#x27;normal&#x27;)),\n",
       "                (&#x27;lgbmclassifier&#x27;,\n",
       "                 LGBMClassifier(colsample_bytree=0.568, learning_rate=0.07,\n",
       "                                max_bin=1023, min_child_samples=864,\n",
       "                                n_estimators=20000, n_jobs=-1, num_leaves=23,\n",
       "                                objective=&#x27;binary&#x27;, random_state=1920,\n",
       "                                reg_alpha=0.65, reg_lambda=3.12))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuantileTransformer</label><div class=\"sk-toggleable__content\"><pre>QuantileTransformer(output_distribution=&#x27;normal&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.568, learning_rate=0.07, max_bin=1023,\n",
       "               min_child_samples=864, n_estimators=20000, n_jobs=-1,\n",
       "               num_leaves=23, objective=&#x27;binary&#x27;, random_state=1920,\n",
       "               reg_alpha=0.65, reg_lambda=3.12)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('quantiletransformer',\n",
       "                 QuantileTransformer(output_distribution='normal')),\n",
       "                ('lgbmclassifier',\n",
       "                 LGBMClassifier(colsample_bytree=0.568, learning_rate=0.07,\n",
       "                                max_bin=1023, min_child_samples=864,\n",
       "                                n_estimators=20000, n_jobs=-1, num_leaves=23,\n",
       "                                objective='binary', random_state=1920,\n",
       "                                reg_alpha=0.65, reg_lambda=3.12))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = pd.DataFrame(QuantileTransformer(output_distribution='normal').fit_transform(X))\n",
    "\n",
    "pipeline = make_pipeline(QuantileTransformer(output_distribution='normal'), model)\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 20757, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8455\n",
      "[LightGBM] [Info] Number of data points in the train set: 91586, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226639 -> initscore=-1.227385\n",
      "[LightGBM] [Info] Start training from score -1.227385\n",
      "[LightGBM] [Info] Number of positive: 20757, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8461\n",
      "[LightGBM] [Info] Number of data points in the train set: 91586, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226639 -> initscore=-1.227385\n",
      "[LightGBM] [Info] Start training from score -1.227385\n",
      "[LightGBM] [Info] Number of positive: 20757, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8450\n",
      "[LightGBM] [Info] Number of data points in the train set: 91586, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226639 -> initscore=-1.227385\n",
      "[LightGBM] [Info] Start training from score -1.227385\n",
      "[LightGBM] [Info] Number of positive: 20758, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8441\n",
      "[LightGBM] [Info] Number of data points in the train set: 91587, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226648 -> initscore=-1.227337\n",
      "[LightGBM] [Info] Start training from score -1.227337\n",
      "[LightGBM] [Info] Number of positive: 20758, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8469\n",
      "[LightGBM] [Info] Number of data points in the train set: 91587, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226648 -> initscore=-1.227337\n",
      "[LightGBM] [Info] Start training from score -1.227337\n",
      "[LightGBM] [Info] Number of positive: 20758, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8457\n",
      "[LightGBM] [Info] Number of data points in the train set: 91587, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226648 -> initscore=-1.227337\n",
      "[LightGBM] [Info] Start training from score -1.227337\n",
      "[LightGBM] [Info] Number of positive: 20758, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8461\n",
      "[LightGBM] [Info] Number of data points in the train set: 91587, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226648 -> initscore=-1.227337\n",
      "[LightGBM] [Info] Start training from score -1.227337\n",
      "[LightGBM] [Info] Number of positive: 20758, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8465\n",
      "[LightGBM] [Info] Number of data points in the train set: 91587, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226648 -> initscore=-1.227337\n",
      "[LightGBM] [Info] Start training from score -1.227337\n",
      "[LightGBM] [Info] Number of positive: 20758, number of negative: 70829\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8447\n",
      "[LightGBM] [Info] Number of data points in the train set: 91587, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226648 -> initscore=-1.227337\n",
      "[LightGBM] [Info] Start training from score -1.227337\n",
      "[LightGBM] [Info] Number of positive: 20757, number of negative: 70830\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8455\n",
      "[LightGBM] [Info] Number of data points in the train set: 91587, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226637 -> initscore=-1.227399\n",
      "[LightGBM] [Info] Start training from score -1.227399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.746201782733674"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline, X, y, scoring='roc_auc', cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id   defects\n",
      "0      101763  0.764512\n",
      "1      101764  0.636366\n",
      "2      101765  0.625447\n",
      "3      101766  0.696000\n",
      "4      101767  0.150330\n",
      "...       ...       ...\n",
      "67837  169600  0.302343\n",
      "67838  169601  0.139168\n",
      "67839  169602  0.186478\n",
      "67840  169603  0.175149\n",
      "67841  169604  0.687957\n",
      "\n",
      "[67842 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_X = pd.read_csv(\"./test.csv\").drop('id',axis=1)\n",
    "results = pipeline.predict_proba(test_X)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] =  np.arange(101763,101763 + len(results))\n",
    "submission['defects'] = results\n",
    "\n",
    "print(submission)\n",
    "submission.to_csv(\"./submission3.0.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
